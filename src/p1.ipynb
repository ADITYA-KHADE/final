{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (4.44.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (0.12.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (4.67.1)\n",
      "Requirement already satisfied: nlpaug in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (1.1.11)\n",
      "Requirement already satisfied: nltk in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (3.9.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (3.3.2)\n",
      "Requirement already satisfied: tf-keras in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (2.18.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (1.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2022.2.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (4.37.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: gdown>=4.0.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from nlpaug) (5.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from gdown>=4.0.0->nlpaug) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\python310\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (63.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.8.1)\n",
      "Requirement already satisfied: namex in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.14.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.3.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers scikit-learn pandas numpy matplotlib seaborn tqdm nlpaug nltk datasets tf-keras accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0           0      3            0                   0        3      2   \n",
      "1           1      3            0                   3        0      1   \n",
      "2           2      3            0                   3        0      1   \n",
      "3           3      3            0                   2        1      1   \n",
      "4           4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Unnamed: 0          100 non-null    int64 \n",
      " 1   count               100 non-null    int64 \n",
      " 2   hate_speech         100 non-null    int64 \n",
      " 3   offensive_language  100 non-null    int64 \n",
      " 4   neither             100 non-null    int64 \n",
      " 5   class               100 non-null    int64 \n",
      " 6   tweet               100 non-null    object\n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 5.6+ KB\n",
      "None\n",
      "       Unnamed: 0       count  hate_speech  offensive_language     neither  \\\n",
      "count  100.000000  100.000000   100.000000          100.000000  100.000000   \n",
      "mean    49.640000    3.120000     0.210000            2.660000    0.250000   \n",
      "std     29.222421    0.728635     0.498381            0.976698    0.687184   \n",
      "min      0.000000    3.000000     0.000000            0.000000    0.000000   \n",
      "25%     24.750000    3.000000     0.000000            2.000000    0.000000   \n",
      "50%     49.500000    3.000000     0.000000            3.000000    0.000000   \n",
      "75%     74.250000    3.000000     0.000000            3.000000    0.000000   \n",
      "max    100.000000    9.000000     3.000000            7.000000    3.000000   \n",
      "\n",
      "            class  \n",
      "count  100.000000  \n",
      "mean     1.050000  \n",
      "std      0.297294  \n",
      "min      0.000000  \n",
      "25%      1.000000  \n",
      "50%      1.000000  \n",
      "75%      1.000000  \n",
      "max      2.000000  \n",
      "Unnamed: 0            0\n",
      "count                 0\n",
      "hate_speech           0\n",
      "offensive_language    0\n",
      "neither               0\n",
      "class                 0\n",
      "tweet                 0\n",
      "dtype: int64\n",
      "label\n",
      "0    98\n",
      "1     2\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    98\n",
      "1    98\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Mohan Khade\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Data Loading and Inspection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "import string\n",
    "from nlpaug.augmenter.word import SynonymAug\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "data_path = '../../Data/Dataset_2.csv'\n",
    "df = pd.read_csv(data_path, encoding='latin1', delimiter=',', quotechar='\"')\n",
    "\n",
    "# Inspect the dataset\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(subset=['tweet'], inplace=True)\n",
    "\n",
    "# Map classes to binary labels\n",
    "df['label'] = df['class'].apply(lambda x: 1 if x == 0 else 0)  # 1: Hate Speech, 0: Non-Hate Speech\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df[['tweet', 'label']]\n",
    "\n",
    "# Check class distribution\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Data Preprocessing\n",
    "# Check your preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Make sure this function matches EXACTLY what you used during training\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df['clean_tweet'] = df['tweet'].apply(preprocess_text)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df[df.label == 0]\n",
    "df_minority = df[df.label == 1]\n",
    "\n",
    "# For upsampling the minority class (recommended when data is limited)\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                               replace=True,     \n",
    "                               n_samples=len(df_majority),    \n",
    "                               random_state=42)  \n",
    "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Verify balanced class distribution\n",
    "print(df_balanced['label'].value_counts())\n",
    "\n",
    "# Tokenization\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to tokenize and encode data\n",
    "def encode_data(texts, tokenizer, max_length=128):\n",
    "    encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "    return encodings\n",
    "\n",
    "# Encode training and testing data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_balanced['clean_tweet'], df_balanced['label'], test_size=0.2, random_state=42, stratify=df_balanced['label']\n",
    ")\n",
    "\n",
    "train_encodings = encode_data(train_texts, tokenizer)\n",
    "val_encodings = encode_data(val_texts, tokenizer)\n",
    "\n",
    "# Dataset Creation for BERT\n",
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HateSpeechDataset(train_encodings, train_labels)\n",
    "val_dataset = HateSpeechDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Aditya Mohan Khade\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 10%|█         | 10/100 [00:38<04:50,  3.22s/it]\n",
      " 10%|█         | 10/100 [00:40<04:50,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31874880194664, 'eval_runtime': 1.1731, 'eval_samples_per_second': 34.098, 'eval_steps_per_second': 2.557, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [01:08<03:26,  2.58s/it]\n",
      " 20%|██        | 20/100 [01:09<03:26,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09574200958013535, 'eval_runtime': 0.9606, 'eval_samples_per_second': 41.642, 'eval_steps_per_second': 3.123, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [01:35<02:54,  2.50s/it]\n",
      " 30%|███       | 30/100 [01:36<02:54,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.010865624994039536, 'eval_runtime': 0.9663, 'eval_samples_per_second': 41.394, 'eval_steps_per_second': 3.105, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [02:02<02:29,  2.50s/it]\n",
      " 40%|████      | 40/100 [02:03<02:29,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.001685402006842196, 'eval_runtime': 0.96, 'eval_samples_per_second': 41.668, 'eval_steps_per_second': 3.125, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [02:28<02:02,  2.44s/it]\n",
      " 50%|█████     | 50/100 [02:29<02:02,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0006598980398848653, 'eval_runtime': 0.9346, 'eval_samples_per_second': 42.797, 'eval_steps_per_second': 3.21, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [02:55<01:38,  2.47s/it]\n",
      " 60%|██████    | 60/100 [02:56<01:38,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0004036347381770611, 'eval_runtime': 0.9844, 'eval_samples_per_second': 40.633, 'eval_steps_per_second': 3.048, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [03:21<01:13,  2.47s/it]\n",
      " 70%|███████   | 70/100 [03:22<01:13,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.000332543917465955, 'eval_runtime': 0.9639, 'eval_samples_per_second': 41.498, 'eval_steps_per_second': 3.112, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [03:48<00:49,  2.47s/it]\n",
      " 80%|████████  | 80/100 [03:49<00:49,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0003006923361681402, 'eval_runtime': 0.9529, 'eval_samples_per_second': 41.977, 'eval_steps_per_second': 3.148, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [04:14<00:24,  2.45s/it]\n",
      " 90%|█████████ | 90/100 [04:15<00:24,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00028606070554815233, 'eval_runtime': 0.9904, 'eval_samples_per_second': 40.388, 'eval_steps_per_second': 3.029, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:41<00:00,  2.48s/it]\n",
      "100%|██████████| 100/100 [04:43<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00028096893220208585, 'eval_runtime': 1.0171, 'eval_samples_per_second': 39.329, 'eval_steps_per_second': 2.95, 'epoch': 10.0}\n",
      "{'train_runtime': 283.8578, 'train_samples_per_second': 5.496, 'train_steps_per_second': 0.352, 'train_loss': 0.08058684349060058, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0, 'eval_runtime': 4.4536, 'eval_samples_per_second': 8.981, 'eval_steps_per_second': 0.674, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Non-Hate Speech       1.00      1.00      1.00        20\n",
      "    Hate Speech       1.00      1.00      1.00        20\n",
      "\n",
      "       accuracy                           1.00        40\n",
      "      macro avg       1.00      1.00      1.00        40\n",
      "   weighted avg       1.00      1.00      1.00        40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# BERT Model Training\n",
    "# Initialize model\n",
    "model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, use_cache=False, gradient_checkpointing=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args_bert = TrainingArguments(\n",
    "    output_dir='./results_bert',\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    fp16=True, \n",
    "    fp16_full_eval=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer_bert = Trainer(\n",
    "    model=model_bert,\n",
    "    args=training_args_bert,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer_bert.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results_bert = trainer_bert.evaluate()\n",
    "print(results_bert)\n",
    "\n",
    "# BERT Model Evaluation\n",
    "# Predict on validation set\n",
    "predictions_bert = trainer_bert.predict(val_dataset)\n",
    "pred_labels_bert = predictions_bert.predictions.argmax(-1)\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(val_labels, pred_labels_bert, target_names=['Non-Hate Speech', 'Hate Speech']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM + CNN Model Definition\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM_CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, lstm_hidden_dim, cnn_hidden_dim, num_classes, dropout=0.5):\n",
    "        super(LSTM_CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, lstm_hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.conv1d = nn.Conv1d(lstm_hidden_dim*2, cnn_hidden_dim, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(cnn_hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Handle both float and long inputs\n",
    "        if x.dtype == torch.float32:\n",
    "            x = x.long()\n",
    "        \n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(embedded)  # [batch_size, seq_len, lstm_hidden_dim*2]\n",
    "        \n",
    "        # Reshape for CNN: [batch_size, lstm_hidden_dim*2, seq_len]\n",
    "        lstm_out = lstm_out.permute(0, 2, 1)\n",
    "        \n",
    "        # Apply CNN\n",
    "        conv_out = F.relu(self.conv1d(lstm_out))  # [batch_size, cnn_hidden_dim, seq_len]\n",
    "        \n",
    "        # Pooling\n",
    "        pooled = self.pool(conv_out).squeeze(-1)  # [batch_size, cnn_hidden_dim]\n",
    "        \n",
    "        # Dropout and final classification\n",
    "        dropped = self.dropout(pooled)\n",
    "        output = self.fc(dropped)  # [batch_size, num_classes]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary Building and Text Sequencing\n",
    "from collections import Counter\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(texts, max_vocab_size=10000):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        tokens = text.split()  # Simple whitespace-based tokenization\n",
    "        counter.update(tokens)\n",
    "    vocab = {word: idx + 1 for idx, (word, _) in enumerate(counter.most_common(max_vocab_size))}\n",
    "    vocab[' '] = 0  # Padding token\n",
    "    return vocab\n",
    "\n",
    "# Convert text to sequences\n",
    "def text_to_sequence(text, vocab, max_len=100):\n",
    "    tokens = text.split()\n",
    "    sequence = [vocab.get(token, 0) for token in tokens]  # Use 0 for unknown tokens\n",
    "    sequence = sequence[:max_len]  # Truncate if longer than max_len \n",
    "    sequence += [0] * (max_len - len(sequence))  # Pad if shorter than max_len\n",
    "    return sequence\n",
    "\n",
    "# Build vocabulary from the dataset\n",
    "vocab = build_vocab(df_balanced['clean_tweet'], max_vocab_size=10000)\n",
    "\n",
    "# Convert all texts to sequences\n",
    "X_sequences = [text_to_sequence(text, vocab, max_len=100) for text in df_balanced['clean_tweet']]\n",
    "X_sequences = torch.tensor(X_sequences, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Split data (convert labels to NumPy array)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_sequences, \n",
    "    df_balanced['label'].values,  # Convert to NumPy array\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df_balanced['label']\n",
    ")\n",
    "\n",
    "# Create datasets for LSTM + CNN\n",
    "class HateSpeechDatasetLSTM_CNN(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        # Convert labels to a numpy array if it's a pandas Series\n",
    "        self.labels = torch.tensor(labels.to_numpy() if isinstance(labels, pd.Series) else labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# Create datasets for LSTM + CNN\n",
    "train_dataset_lstm_cnn = HateSpeechDatasetLSTM_CNN(X_train, y_train)\n",
    "val_dataset_lstm_cnn = HateSpeechDatasetLSTM_CNN(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.4996066361665726\n",
      "Validation Loss: 0.2966541697581609, Accuracy: 1.0\n",
      "Epoch 2/10, Loss: 0.15125424563884735\n",
      "Validation Loss: 0.057566589365402855, Accuracy: 1.0\n",
      "Epoch 3/10, Loss: 0.017804408888332547\n",
      "Validation Loss: 0.005842453179260095, Accuracy: 1.0\n",
      "Epoch 4/10, Loss: 0.0019964916107710453\n",
      "Validation Loss: 0.0011778951350909967, Accuracy: 1.0\n",
      "Epoch 5/10, Loss: 0.0003936199733288959\n",
      "Validation Loss: 0.0006650499708484858, Accuracy: 1.0\n",
      "Epoch 6/10, Loss: 0.00020753161661559715\n",
      "Validation Loss: 0.0004982521156004319, Accuracy: 1.0\n",
      "Epoch 7/10, Loss: 0.00014954792277421802\n",
      "Validation Loss: 0.0003876340148660044, Accuracy: 1.0\n",
      "Epoch 8/10, Loss: 0.0001228118529979838\n",
      "Validation Loss: 0.0003058744720571364, Accuracy: 1.0\n",
      "Epoch 9/10, Loss: 0.00010806382197188213\n",
      "Validation Loss: 0.00026091705755485844, Accuracy: 1.0\n",
      "Epoch 10/10, Loss: 0.00010269130652886815\n",
      "Validation Loss: 0.00023960685211932287, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters for LSTM + CNN\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 100\n",
    "lstm_hidden_dim = 128\n",
    "cnn_hidden_dim = 128\n",
    "num_classes = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# Initialize model for LSTM + CNN\n",
    "model_lstm_cnn = LSTM_CNN(vocab_size, embed_dim, lstm_hidden_dim, cnn_hidden_dim, num_classes, dropout)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_lstm_cnn.parameters(), lr=1e-3)\n",
    "\n",
    "# Define training loop for LSTM + CNN\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, labels = batch\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader)}')\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids, labels = batch\n",
    "                outputs = model(input_ids)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader)}, Accuracy: {correct/total}')\n",
    "\n",
    "# Create DataLoader for LSTM + CNN\n",
    "train_loader = DataLoader(train_dataset_lstm_cnn, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset_lstm_cnn, batch_size=16, shuffle=False)\n",
    "\n",
    "# Train the LSTM + CNN model\n",
    "train_model(model_lstm_cnn, criterion, optimizer, train_loader, val_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aditya Mohan\n",
      "[nltk_data]     Khade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Aditya Mohan\n",
      "[nltk_data]     Khade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Aditya Mohan\n",
      "[nltk_data]     Khade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented texts: 980, Augmented labels: 980\n",
      "Epoch 1/10, Loss: 0.1318439114545334\n",
      "Validation Loss: 0.0005439736346791809, Accuracy: 1.0\n",
      "Epoch 2/10, Loss: 0.00023591931001307892\n",
      "Validation Loss: 0.00012516919863022244, Accuracy: 1.0\n",
      "Epoch 3/10, Loss: 6.658243393172536e-05\n",
      "Validation Loss: 8.583356199475626e-05, Accuracy: 1.0\n",
      "Epoch 4/10, Loss: 1.875829103112865e-05\n",
      "Validation Loss: 1.0563711839495227e-05, Accuracy: 1.0\n",
      "Epoch 5/10, Loss: 5.45374959005847e-06\n",
      "Validation Loss: 7.226347088362673e-06, Accuracy: 1.0\n",
      "Epoch 6/10, Loss: 4.7166689572057265e-06\n",
      "Validation Loss: 6.3530472743877905e-06, Accuracy: 1.0\n",
      "Epoch 7/10, Loss: 4.140167652236661e-06\n",
      "Validation Loss: 5.341885237915752e-06, Accuracy: 1.0\n",
      "Epoch 8/10, Loss: 3.60922817803531e-06\n",
      "Validation Loss: 4.9910225849695655e-06, Accuracy: 1.0\n",
      "Epoch 9/10, Loss: 3.1542053239579635e-06\n",
      "Validation Loss: 4.296358080561428e-06, Accuracy: 1.0\n",
      "Epoch 10/10, Loss: 2.7658589191313526e-06\n",
      "Validation Loss: 3.67287726324624e-06, Accuracy: 1.0\n",
      "Epoch 1/10, Loss: 0.000019\n",
      "Validation Loss: 0.000002, Accuracy: 1.0000\n",
      "Epoch 2/10, Loss: 0.000011\n",
      "Validation Loss: 0.000001, Accuracy: 1.0000\n",
      "Epoch 3/10, Loss: 0.000008\n",
      "Validation Loss: 0.000000, Accuracy: 1.0000\n",
      "Epoch 4/10, Loss: 0.000003\n",
      "Validation Loss: 0.000000, Accuracy: 1.0000\n",
      "Epoch 5/10, Loss: 0.000003\n",
      "Validation Loss: 0.000000, Accuracy: 1.0000\n",
      "Epoch 6/10, Loss: 0.000003\n",
      "Validation Loss: 0.000000, Accuracy: 1.0000\n",
      "Epoch 7/10, Loss: 0.000002\n",
      "Validation Loss: 0.000000, Accuracy: 1.0000\n",
      "Epoch 8/10, Loss: 0.000002\n",
      "Validation Loss: 0.000000, Accuracy: 1.0000\n",
      "Epoch 9/10, Loss: 0.000001\n",
      "Validation Loss: 0.000000, Accuracy: 1.0000\n",
      "Epoch 10/10, Loss: 0.000001\n",
      "Validation Loss: 0.000000, Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation\n",
    "import nltk\n",
    "import copy\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nlpaug.augmenter.word import SynonymAug\n",
    "\n",
    "aug = SynonymAug(aug_src='wordnet')\n",
    "\n",
    "def augment_text(text, augmenter, num_aug=5):\n",
    "    try:\n",
    "        augmented = augmenter.augment(text, n=num_aug)\n",
    "        return augmented if augmented else []\n",
    "    except Exception as e:\n",
    "        print(f\"Augmentation failed for text: {text}. Error: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Augment the dataset\n",
    "augmented_texts = []\n",
    "augmented_labels = []\n",
    "\n",
    "# Reset index to ensure contiguous integer indexing\n",
    "df_balanced = df_balanced.reset_index(drop=True)  # Critical fix\n",
    "\n",
    "for i, text in enumerate(df_balanced['clean_tweet']):\n",
    "    try:\n",
    "        augmented = augment_text(text, aug)\n",
    "        if len(augmented) > 0:\n",
    "            augmented_texts.extend(augmented)\n",
    "            augmented_labels.extend([df_balanced['label'].iloc[i]] * len(augmented))\n",
    "    except Exception as e:\n",
    "        print(f\"Error augmenting text: {text}. Error: {e}\")\n",
    "\n",
    "# Verify lengths match\n",
    "print(f\"Augmented texts: {len(augmented_texts)}, Augmented labels: {len(augmented_labels)}\")\n",
    "\n",
    "# Create augmented DataFrame\n",
    "df_augmented = pd.DataFrame({'clean_tweet': augmented_texts, 'label': augmented_labels})\n",
    "\n",
    "# Combine and split data\n",
    "df_combined = pd.concat([df_balanced, df_augmented]).reset_index(drop=True)\n",
    "\n",
    "# Convert combined texts to sequences\n",
    "X_combined = [text_to_sequence(text, vocab, max_len=100) for text in df_combined['clean_tweet']]\n",
    "X_combined = torch.tensor(X_combined, dtype=torch.long)\n",
    "\n",
    "# Split data with error handling\n",
    "try:\n",
    "    X_train_combined, X_val_combined, y_train_combined, y_val_combined = train_test_split(\n",
    "        X_combined, df_combined['label'], test_size=0.2, random_state=42, stratify=df_combined['label']\n",
    "    )\n",
    "except ValueError:\n",
    "    # Fallback if stratify fails\n",
    "    X_train_combined, X_val_combined, y_train_combined, y_val_combined = train_test_split(\n",
    "        X_combined, df_combined['label'], test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "# Create datasets for LSTM + CNN\n",
    "train_dataset_combined = HateSpeechDatasetLSTM_CNN(X_train_combined, y_train_combined)\n",
    "val_dataset_combined = HateSpeechDatasetLSTM_CNN(X_val_combined, y_val_combined)\n",
    "\n",
    "# DataLoader\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=16, shuffle=True)\n",
    "val_loader_combined = DataLoader(val_dataset_combined, batch_size=16, shuffle=False)\n",
    "\n",
    "# Define hyperparameters for LSTM + CNN\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 100\n",
    "lstm_hidden_dim = 128\n",
    "cnn_hidden_dim = 128\n",
    "num_classes = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# Initialize model for LSTM + CNN\n",
    "model_lstm_cnn = LSTM_CNN(vocab_size, embed_dim, lstm_hidden_dim, cnn_hidden_dim, num_classes, dropout)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_lstm_cnn.parameters(), lr=1e-3)\n",
    "\n",
    "# Define training loop for LSTM + CNN\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, labels = batch\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader)}')\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids, labels = batch\n",
    "                outputs = model(input_ids)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print(f'Validation Loss: {val_loss/len(val_loader)}, Accuracy: {correct/total}')\n",
    "\n",
    "# Train with augmented data\n",
    "train_model(model_lstm_cnn, criterion, optimizer, train_loader_combined, val_loader_combined, num_epochs=10)\n",
    "\n",
    "# Adversarial Training\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def adversarial_train(model, criterion, optimizer, train_loader, val_loader, epsilon=0.01, num_epochs=10):\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids, labels = batch\n",
    "            input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "            \n",
    "            # STEP 1: Regular forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward(retain_graph=True)\n",
    "            \n",
    "            # STEP 2: Generate adversarial examples\n",
    "            # Get embedding gradients\n",
    "            emb_grad = None\n",
    "            for name, param in model.named_parameters():\n",
    "                if name == 'embedding.weight' and param.grad is not None:\n",
    "                    emb_grad = param.grad\n",
    "            \n",
    "            if emb_grad is not None:\n",
    "                # Create adversarial perturbation at the embedding level\n",
    "                embedded = model.embedding(input_ids)\n",
    "                embedded.retain_grad()\n",
    "                \n",
    "                # This could be improved with better gradient handling\n",
    "                delta = epsilon * torch.sign(torch.index_select(emb_grad, 0, input_ids.view(-1)))\n",
    "                delta = delta.view(input_ids.size(0), input_ids.size(1), -1)\n",
    "                \n",
    "                # Get perturbed embeddings\n",
    "                embedded_adv = embedded.detach() + delta\n",
    "                \n",
    "                # Manual forward pass with perturbed embedding\n",
    "                lstm_out, _ = model.lstm(embedded_adv)\n",
    "                lstm_out = lstm_out.permute(0, 2, 1)\n",
    "                conv_out = F.relu(model.conv1d(lstm_out))\n",
    "                pooled = model.pool(conv_out).squeeze(-1)\n",
    "                dropped = model.dropout(pooled)\n",
    "                adv_outputs = model.fc(dropped)\n",
    "                \n",
    "                # Adversarial loss\n",
    "                adv_loss = criterion(adv_outputs, labels)\n",
    "                \n",
    "                # Combine loss\n",
    "                total_loss = loss + adv_loss\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += total_loss.item()\n",
    "            else:\n",
    "                # If no gradients, just use regular loss\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}')\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids, labels = batch\n",
    "                input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "                outputs = model(input_ids)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_avg_loss = val_loss / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        print(f'Validation Loss: {val_avg_loss:.6f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Adversarial train LSTM + CNN\n",
    "adversarial_train(model_lstm_cnn, criterion, optimizer, train_loader_combined, val_loader_combined, epsilon=0.01, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Adversarial Model Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Non-Hate Speech       1.00      1.00      1.00       118\n",
      "    Hate Speech       1.00      1.00      1.00       118\n",
      "\n",
      "       accuracy                           1.00       236\n",
      "      macro avg       1.00      1.00      1.00       236\n",
      "   weighted avg       1.00      1.00      1.00       236\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, add this line to define the device variable before using it\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Predict on validation set using the adversarially trained model\n",
    "model_lstm_cnn.eval()  # Use model_lstm_cnn instead of model_adv\n",
    "preds_list_adv = []\n",
    "true_labels_adv = []\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader_combined:  # Use val_loader_combined instead of val_loader_adv\n",
    "            # For standard PyTorch dataloaders which return (inputs, labels) tuples\n",
    "            input_ids, labels = batch\n",
    "            input_ids, labels = input_ids.to(device), labels.to(device)\n",
    "            true_labels_adv.extend(labels.cpu().numpy())\n",
    "            \n",
    "            outputs = model_lstm_cnn(input_ids)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "                \n",
    "            # Collect predictions\n",
    "            preds_list_adv.extend(preds.cpu().numpy())\n",
    "            \n",
    "    # Generate classification report\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    \n",
    "    print(f\"Adversarial Model Accuracy: {accuracy_score(true_labels_adv, preds_list_adv):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels_adv, preds_list_adv, target_names=['Non-Hate Speech', 'Hate Speech']))\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Just plot the classification report for the adversarial model which you already have\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function to plot classification report\n",
    "def plot_classification_report(report, title):\n",
    "    report_df = pd.DataFrame(report).iloc[:-1, :].T  \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(report_df, annot=True, fmt=\".2f\", cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# You already have true_labels_adv and preds_list_adv from your model evaluation\n",
    "report = classification_report(true_labels_adv, preds_list_adv, \n",
    "                             target_names=['Non-Hate Speech', 'Hate Speech'], \n",
    "                             output_dict=True)\n",
    "                             \n",
    "plot_classification_report(report, \"Adversarial Model Performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the BERT model (which has save_pretrained)\n",
    "model_bert.save_pretrained('./hate_speech_model_bert')\n",
    "tokenizer.save_pretrained('./hate_speech_model_bert')\n",
    "\n",
    "# Save the LSTM_CNN model using PyTorch's native saving method\n",
    "model_info = {\n",
    "    'state_dict': model_lstm_cnn.state_dict(),\n",
    "    'vocab_size': len(vocab),\n",
    "    'embed_dim': 100,\n",
    "    'lstm_hidden_dim': 128,\n",
    "    'cnn_hidden_dim': 128,\n",
    "    'num_classes': 2,\n",
    "    'dropout': 0.5\n",
    "}\n",
    "torch.save(model_info, './hate_speech_model_lstm_cnn.pth')\n",
    "\n",
    "# Save the vocabulary for LSTM_CNN model\n",
    "import pickle\n",
    "with open('./vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "print(\"Models saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Loading the BERT model\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "loaded_bert_model = BertForSequenceClassification.from_pretrained('./hate_speech_model_bert')\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained('./hate_speech_model_bert')\n",
    "\n",
    "# Loading the LSTM_CNN model\n",
    "# First define the model architecture with the same parameters\n",
    "vocab_size = len(vocab)  # You need to know or load the vocab size\n",
    "embed_dim = 100  # Use the same dimensions as during training\n",
    "lstm_hidden_dim = 128\n",
    "cnn_hidden_dim = 128\n",
    "num_classes = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# When loading:\n",
    "model_info = torch.load('./hate_speech_model_lstm_cnn.pth')\n",
    "loaded_lstm_cnn = LSTM_CNN(\n",
    "    model_info['vocab_size'], \n",
    "    model_info['embed_dim'], \n",
    "    model_info['lstm_hidden_dim'],\n",
    "    model_info['cnn_hidden_dim'],\n",
    "    model_info['num_classes'],\n",
    "    model_info['dropout']\n",
    ")\n",
    "loaded_lstm_cnn.load_state_dict(model_info['state_dict'])\n",
    "loaded_lstm_cnn.eval()  # Set to evaluation mode\n",
    "\n",
    "# Load the vocabulary\n",
    "with open('./vocab.pkl', 'rb') as f:\n",
    "    loaded_vocab = pickle.load(f)\n",
    "\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load vocabulary from: a:\\8TH SEMESTER\\Major Project\\new\\1\\vocab.pkl\n",
      "Vocabulary loaded successfully with 489 entries\n",
      "Attempting to load model from: a:\\8TH SEMESTER\\Major Project\\new\\1\\hate_speech_model_lstm_cnn.pth\n",
      "Model and vocabulary loaded successfully!\n",
      "Starting Flask API server...\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [28/Feb/2025 15:56:41] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Feb/2025 15:56:43] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Feb/2025 15:56:43] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Feb/2025 15:56:48] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Kill all [slur] people they don't deserve to live\n",
      "Preprocessed: kill all slur people they dont deserve to live\n",
      "Vocab coverage: 5/9 tokens\n",
      "Raw logits: [[10.3920145 -8.606645 ]]\n",
      "Probabilities: [[1.000000e+00 5.610314e-09]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Feb/2025 15:56:49] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Kill all [slur] people they don't deserve to live\n",
      "Preprocessed: kill all slur people they dont deserve to live\n",
      "Vocab coverage: 5/9 tokens\n",
      "Raw logits: [[10.3920145 -8.606645 ]]\n",
      "Probabilities: [[1.000000e+00 5.610314e-09]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Feb/2025 15:57:14] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Kill all [slur] people they don't deserve to live\n",
      "Preprocessed: kill all slur people they dont deserve to live\n",
      "Vocab coverage: 5/9 tokens\n",
      "Raw logits: [[10.3920145 -8.606645 ]]\n",
      "Probabilities: [[1.000000e+00 5.610314e-09]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Feb/2025 15:57:58] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: We should eliminate all people who follow that religion to make our country pure again.\n",
      "Preprocessed: we should eliminate all people who follow that religion to make our country pure again\n",
      "Vocab coverage: 7/15 tokens\n",
      "Raw logits: [[ 12.162737 -10.144196]]\n",
      "Probabilities: [[1.0000000e+00 2.0522126e-10]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Feb/2025 15:57:59] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: We should eliminate all people who follow that religion to make our country pure again.\n",
      "Preprocessed: we should eliminate all people who follow that religion to make our country pure again\n",
      "Vocab coverage: 7/15 tokens\n",
      "Raw logits: [[ 12.162737 -10.144196]]\n",
      "Probabilities: [[1.0000000e+00 2.0522126e-10]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Feb/2025 15:57:59] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: We should eliminate all people who follow that religion to make our country pure again.\n",
      "Preprocessed: we should eliminate all people who follow that religion to make our country pure again\n",
      "Vocab coverage: 7/15 tokens\n",
      "Raw logits: [[ 12.162737 -10.144196]]\n",
      "Probabilities: [[1.0000000e+00 2.0522126e-10]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Feb/2025 15:57:59] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Feb/2025 15:58:00] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: We should eliminate all people who follow that religion to make our country pure again.\n",
      "Preprocessed: we should eliminate all people who follow that religion to make our country pure again\n",
      "Vocab coverage: 7/15 tokens\n",
      "Raw logits: [[ 12.162737 -10.144196]]\n",
      "Probabilities: [[1.0000000e+00 2.0522126e-10]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n",
      "Input text: We should eliminate all people who follow that religion to make our country pure again.\n",
      "Preprocessed: we should eliminate all people who follow that religion to make our country pure again\n",
      "Vocab coverage: 7/15 tokens\n",
      "Raw logits: [[ 12.162737 -10.144196]]\n",
      "Probabilities: [[1.0000000e+00 2.0522126e-10]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Feb/2025 15:58:00] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Feb/2025 15:58:00] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: We should eliminate all people who follow that religion to make our country pure again.\n",
      "Preprocessed: we should eliminate all people who follow that religion to make our country pure again\n",
      "Vocab coverage: 7/15 tokens\n",
      "Raw logits: [[ 12.162737 -10.144196]]\n",
      "Probabilities: [[1.0000000e+00 2.0522126e-10]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n",
      "Input text: We should eliminate all people who follow that religion to make our country pure again.\n",
      "Preprocessed: we should eliminate all people who follow that religion to make our country pure again\n",
      "Vocab coverage: 7/15 tokens\n",
      "Raw logits: [[ 12.162737 -10.144196]]\n",
      "Probabilities: [[1.0000000e+00 2.0522126e-10]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Feb/2025 15:58:00] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Feb/2025 15:58:00] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: We should eliminate all people who follow that religion to make our country pure again.\n",
      "Preprocessed: we should eliminate all people who follow that religion to make our country pure again\n",
      "Vocab coverage: 7/15 tokens\n",
      "Raw logits: [[ 12.162737 -10.144196]]\n",
      "Probabilities: [[1.0000000e+00 2.0522126e-10]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n",
      "Input text: We should eliminate all people who follow that religion to make our country pure again.\n",
      "Preprocessed: we should eliminate all people who follow that religion to make our country pure again\n",
      "Vocab coverage: 7/15 tokens\n",
      "Raw logits: [[ 12.162737 -10.144196]]\n",
      "Probabilities: [[1.0000000e+00 2.0522126e-10]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Feb/2025 15:58:01] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: We should eliminate all people who follow that religion to make our country pure again.\n",
      "Preprocessed: we should eliminate all people who follow that religion to make our country pure again\n",
      "Vocab coverage: 7/15 tokens\n",
      "Raw logits: [[ 12.162737 -10.144196]]\n",
      "Probabilities: [[1.0000000e+00 2.0522126e-10]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Feb/2025 15:58:01] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: We should eliminate all people who follow that religion to make our country pure again.\n",
      "Preprocessed: we should eliminate all people who follow that religion to make our country pure again\n",
      "Vocab coverage: 7/15 tokens\n",
      "Raw logits: [[ 12.162737 -10.144196]]\n",
      "Probabilities: [[1.0000000e+00 2.0522126e-10]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Feb/2025 15:58:01] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: We should eliminate all people who follow that religion to make our country pure again.\n",
      "Preprocessed: we should eliminate all people who follow that religion to make our country pure again\n",
      "Vocab coverage: 7/15 tokens\n",
      "Raw logits: [[ 12.162737 -10.144196]]\n",
      "Probabilities: [[1.0000000e+00 2.0522126e-10]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Feb/2025 15:58:16] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: It's natural to want to be with your own kind, it's human nature.\n",
      "Preprocessed: its natural to want to be with your own kind its human nature\n",
      "Vocab coverage: 8/13 tokens\n",
      "Raw logits: [[11.780037 -9.817939]]\n",
      "Probabilities: [[1.0000000e+00 4.1698256e-10]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Feb/2025 15:58:16] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Feb/2025 15:58:16] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: It's natural to want to be with your own kind, it's human nature.\n",
      "Preprocessed: its natural to want to be with your own kind its human nature\n",
      "Vocab coverage: 8/13 tokens\n",
      "Raw logits: [[11.780037 -9.817939]]\n",
      "Probabilities: [[1.0000000e+00 4.1698256e-10]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n",
      "Input text: It's natural to want to be with your own kind, it's human nature.\n",
      "Preprocessed: its natural to want to be with your own kind its human nature\n",
      "Vocab coverage: 8/13 tokens\n",
      "Raw logits: [[11.780037 -9.817939]]\n",
      "Probabilities: [[1.0000000e+00 4.1698256e-10]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [28/Feb/2025 15:58:17] \"POST /predict_lstm_cnn HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: It's natural to want to be with your own kind, it's human nature.\n",
      "Preprocessed: its natural to want to be with your own kind its human nature\n",
      "Vocab coverage: 8/13 tokens\n",
      "Raw logits: [[11.780037 -9.817939]]\n",
      "Probabilities: [[1.0000000e+00 4.1698256e-10]]\n",
      "Prediction: {'label': 'Non-Hate Speech', 'score': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, current_app\n",
    "import torch\n",
    "import torch.nn as nn  # Missing import for nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import os  # Missing import\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model definition\n",
    "class LSTM_CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, lstm_hidden_dim, cnn_hidden_dim, num_classes, dropout=0.5):\n",
    "        super(LSTM_CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, lstm_hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.conv1d = nn.Conv1d(lstm_hidden_dim*2, cnn_hidden_dim, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(cnn_hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Handle both float and long inputs\n",
    "        if x.dtype == torch.float32:\n",
    "            x = x.long()\n",
    "        \n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(embedded)  # [batch_size, seq_len, lstm_hidden_dim*2]\n",
    "        \n",
    "        # Reshape for CNN: [batch_size, lstm_hidden_dim*2, seq_len]\n",
    "        lstm_out = lstm_out.permute(0, 2, 1)\n",
    "        \n",
    "        # Apply CNN\n",
    "        conv_out = F.relu(self.conv1d(lstm_out))  # [batch_size, cnn_hidden_dim, seq_len]\n",
    "        \n",
    "        # Pooling\n",
    "        pooled = self.pool(conv_out).squeeze(-1)  # [batch_size, cnn_hidden_dim]\n",
    "        \n",
    "        # Dropout and final classification\n",
    "        dropped = self.dropout(pooled)\n",
    "        output = self.fc(dropped)  # [batch_size, num_classes]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Load model and vocab outside the request context\n",
    "model_deploy = None\n",
    "vocab = None\n",
    "\n",
    "# Preprocess text function\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove mentions (@user)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags (#hashtag)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Function to convert text to sequence\n",
    "def text_to_sequence(text, vocab_dict, max_len=100):\n",
    "    tokens = text.split()\n",
    "    sequence = [vocab_dict.get(token, 1) for token in tokens]  # Use 1 (<UNK>) for unknown words\n",
    "    # Truncate or pad to fixed length\n",
    "    if len(sequence) > max_len:\n",
    "        sequence = sequence[:max_len]\n",
    "    else:\n",
    "        sequence += [0] * (max_len - len(sequence))\n",
    "    return sequence\n",
    "\n",
    "# Replace before_first_request with a direct initialization function\n",
    "def init_app():\n",
    "    global model_deploy, vocab\n",
    "    try:\n",
    "        # Get current notebook directory and construct paths\n",
    "        current_dir = os.path.abspath('.')\n",
    "        vocab_path = os.path.join(current_dir, 'vocab.pkl')\n",
    "        model_path = os.path.join(current_dir, 'hate_speech_model_lstm_cnn.pth')\n",
    "        \n",
    "        print(f\"Attempting to load vocabulary from: {vocab_path}\")\n",
    "        if not os.path.exists(vocab_path):\n",
    "            print(f\"ERROR: Vocabulary file not found at {vocab_path}\")\n",
    "            return False\n",
    "            \n",
    "        # Load vocabulary from file\n",
    "        with open(vocab_path, 'rb') as f:\n",
    "            vocab = pickle.load(f)\n",
    "        print(f\"Vocabulary loaded successfully with {len(vocab)} entries\")\n",
    "        \n",
    "        print(f\"Attempting to load model from: {model_path}\")\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"ERROR: Model file not found at {model_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Load model parameters\n",
    "        model_info = torch.load(model_path, map_location=device)\n",
    "        \n",
    "        # Initialize model using parameters from the saved model\n",
    "        model_deploy = LSTM_CNN(\n",
    "            model_info['vocab_size'],\n",
    "            model_info['embed_dim'],\n",
    "            model_info['lstm_hidden_dim'],\n",
    "            model_info['cnn_hidden_dim'],\n",
    "            model_info['num_classes'],\n",
    "            model_info['dropout']\n",
    "        )\n",
    "        model_deploy.load_state_dict(model_info['state_dict'])\n",
    "        model_deploy.to(device)\n",
    "        model_deploy.eval()\n",
    "        \n",
    "        print(\"Model and vocabulary loaded successfully!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error loading model or vocabulary: {e}\")\n",
    "        print(traceback.format_exc())  # Print full traceback for debugging\n",
    "        return False\n",
    "\n",
    "# Initialize at startup\n",
    "if not init_app():\n",
    "    raise RuntimeError(\"Failed to initialize model or vocabulary\")\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return \"Hate Speech Detection API\"\n",
    "\n",
    "@app.route('/predict_lstm_cnn', methods=['POST'])\n",
    "def predict_lstm_cnn():\n",
    "    data = request.json\n",
    "    text = data['text']\n",
    "    print(f\"Input text: {text}\")\n",
    "    \n",
    "    # Preprocess text\n",
    "    text_processed = preprocess_text(text)\n",
    "    print(f\"Preprocessed: {text_processed}\")\n",
    "    \n",
    "    # Check vocabulary coverage\n",
    "    tokens = text_processed.split()\n",
    "    known_tokens = [t for t in tokens if t in vocab]\n",
    "    print(f\"Vocab coverage: {len(known_tokens)}/{len(tokens)} tokens\")\n",
    "    \n",
    "    # Convert to sequence\n",
    "    sequence = text_to_sequence(text_processed, vocab, max_len=100)\n",
    "    sequence_tensor = torch.tensor([sequence], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Get prediction with detailed output\n",
    "    with torch.no_grad():\n",
    "        outputs = model_deploy(sequence_tensor)\n",
    "        logits = outputs.detach().cpu().numpy()\n",
    "        print(f\"Raw logits: {logits}\")\n",
    "        \n",
    "        pred_probs = F.softmax(outputs, dim=1)\n",
    "        print(f\"Probabilities: {pred_probs.detach().cpu().numpy()}\")\n",
    "        \n",
    "        pred_label = torch.argmax(pred_probs, dim=1).item()\n",
    "    \n",
    "    result = {\n",
    "        'label': ['Non-Hate Speech', 'Hate Speech'][pred_label],\n",
    "        'score': float(pred_probs[0][pred_label])\n",
    "    }\n",
    "    \n",
    "    print(f\"Prediction: {result}\")\n",
    "    return jsonify(result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Starting Flask API server...\")\n",
    "    app.run(debug=False, threaded=True)  # Use threaded=True to avoid blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Cannot connect to the server. Make sure the Flask app is running.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Sample text for testing\n",
    "test_text = {\"text\": \"I hate that idiot and his stupid ideas\"}\n",
    "\n",
    "# Make a request to your API\n",
    "try:\n",
    "    response = requests.post('http://127.0.0.1:5000/predict_lstm_cnn', \n",
    "                           json=test_text,\n",
    "                           timeout=5)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"Prediction: {result['label']}\")\n",
    "        print(f\"Confidence: {result['score']:.4f}\")\n",
    "    else:\n",
    "        print(f\"Error: Received status code {response.status_code}\")\n",
    "        print(response.text)\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Error: Could not connect to the server. Make sure it's running.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
